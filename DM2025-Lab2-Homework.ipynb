{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Juho Viljanen\n",
    "\n",
    "Student ID: 114300350\n",
    "\n",
    "GitHub ID: viljanenjuho\n",
    "\n",
    "Kaggle name: juhoviljanen1\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/leaderboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "The preprocessing phase focused on preparing noisy, user-generated text for downstream modeling. Because the dataset originated from informal social media posts, it contained emojis, inconsistent punctuation, hashtags, and colloquial expressions. For transformer-based models, minimal preprocessing was applied to preserve these emotionally relevant cues; only URLs, malformed unicode, and redundant whitespace were removed. This approach aligns with modern research indicating that large language models rely on full textual context, including punctuation and emotive symbols, to capture sentiment and emotion. A secondary “cleaned” version of the text was also constructed for classical machine-learning models, involving lowercasing, whitespace normalization, and removal of extraneous characters.\n",
    "\n",
    "The dataset was reconstructed from multiple sources, including raw text in JSON format, an identification file specifying train/test splits, and emotion labels for training instances. These were merged to form coherent training and testing sets. An early distributional analysis revealed substantial class imbalance—most prominently, the “joy” class dominated the dataset with nearly twenty times as many examples as the “disgust” class. This imbalance later proved to be a central factor influencing model performance.\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "Feature engineering techniques were adapted to the modeling strategy. For classical machine-learning models, TF–IDF vectorization was employed to convert text into numerical representations. Unigrams and bigrams were used to capture both individual tokens and short contextual patterns, while sublinear term weighting and a high-capacity vocabulary helped retain expressive detail. These engineered features allowed linear models such as Support Vector Machines (SVMs) to establish a strong baseline.\n",
    "\n",
    "In contrast, transformer-based architectures (DistilBERT, RoBERTa, and DeBERTa) relied primarily on learned contextual embeddings produced by their own tokenizers. No handcrafted features were added to these models, as the deep contextual representations generated by their attention mechanisms serve as end-to-end feature extractors.\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "The final model selected for this project was DeBERTa-v3-Large, a state-of-the-art transformer architecture developed by Microsoft. DeBERTa (“Decoding-Enhanced BERT with Disentangled Attention”) improves over previous transformer models—such as BERT and RoBERTa—by separating content and positional embeddings and using an enhanced mask decoder. These design choices allow the model to better understand subtle emotional signals, including punctuation patterns, emojis, informal phrasing, and other sentiment-rich features common in social-media text. Because emotion detection requires interpreting nuanced linguistic cues, DeBERTa-v3-Large provides a significantly more capable representation than smaller architectures such as DistilBERT or standard BERT.\n",
    "\n",
    "Before fine-tuning, the severe class imbalance in the dataset limited performance, as minority emotions such as disgust and fear were underrepresented. To address this, the training data was oversampled, ensuring that each emotion class had an approximately equal number of training examples. After balancing the dataset, the model was fine-tuned on the emotion classification task using supervised learning, with the loss function optimized to distinguish among six emotional categories. The model’s large capacity, contextual sensitivity, and improved attention mechanisms allow it to learn richer emotional patterns than earlier approaches, resulting in stronger generalization performance across all emotion classes.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "Several strategies were explored to address poor minority-class performance. These included probability-threshold optimization, back-translation-based data augmentation, synonym replacement using lexical resources, and training multiple transformer variants. Threshold optimization produced negligible gains, indicating that the underlying class separability was insufficient. Data augmentation attempts were abandoned due to runtime constraints and implementation issues. Ultimately, oversampling minority classes before model training produced the most practical and reliable improvement.\n",
    "\n",
    "I also compared DistilBERT, RoBERTa-base, and DeBERTa-v3. Although DistilBERT and RoBERTa underperformed due to imbalance, DeBERTa-v3 demonstrated best capacity once class frequencies were corrected.\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "The primary insight from this project was that data quality and class balance can dominate model performance, even when using advanced architectures. Transformers are not inherently robust to extreme imbalance; without sufficient representation of each class, they gravitate toward majority-class predictions and inflate weighted metrics while suppressing macro-level performance. Additionally, minimal text cleaning proved essential for transformer models, as removing punctuation or emojis degraded their ability to capture emotional nuance.\n",
    "\n",
    "A second key insight was that simpler models such as TF–IDF + SVM can outperform advanced models when the dataset is imbalanced or noisy. However, once balanced, transformers exhibit markedly superior performance and generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load raw JSON with posts\n",
    "with open(\"final_posts.json\", \"r\") as f:\n",
    "    posts = json.load(f)\n",
    "\n",
    "posts_df = pd.DataFrame(posts)[[\"id\", \"text\"]]\n",
    "\n",
    "# Load train/test split info\n",
    "split_df = pd.read_csv(\"data_identification.csv\")\n",
    "\n",
    "# Load emotion labels\n",
    "emotion_df = pd.read_csv(\"emotion.csv\")\n",
    "\n",
    "# Merge posts with split info\n",
    "merged = posts_df.merge(split_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# Split into train and test\n",
    "train_df = merged[merged[\"split\"] == \"train\"].merge(emotion_df, on=\"id\", how=\"left\")\n",
    "test_df  = merged[merged[\"split\"] == \"test\"]\n",
    "\n",
    "train_df = train_df[[\"id\", \"text\", \"emotion\"]].reset_index(drop=True)\n",
    "test_df  = test_df[[\"id\", \"text\"]].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary as i focused on different bert models more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import scipy.sparse as sp\n",
    "import emoji\n",
    "\n",
    "labels = sorted(train_df[\"emotion\"].unique())\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "train_df[\"label\"] = train_df[\"emotion\"].map(label2id).astype(int)\n",
    "\n",
    "X_text_train = train_df[\"clean_text\"]\n",
    "X_text_test  = test_df[\"clean_text\"]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=100000,\n",
    "    sublinear_tf=True,\n",
    "    min_df=3,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_text_train)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_text_test)\n",
    "\n",
    "train_df[\"char_len\"] = train_df[\"text\"].str.len()\n",
    "test_df[\"char_len\"]  = test_df[\"text\"].str.len()\n",
    "\n",
    "train_df[\"exclaim_count\"] = train_df[\"text\"].str.count(\"!\")\n",
    "test_df[\"exclaim_count\"]  = test_df[\"text\"].str.count(\"!\")\n",
    "\n",
    "def count_emojis(s):\n",
    "    return sum(1 for ch in str(s) if ch in emoji.EMOJI_DATA)\n",
    "\n",
    "train_df[\"emoji_count\"] = train_df[\"text\"].apply(count_emojis)\n",
    "test_df[\"emoji_count\"]  = test_df[\"text\"].apply(count_emojis)\n",
    "\n",
    "num_features = [\"char_len\", \"exclaim_count\", \"emoji_count\"]\n",
    "\n",
    "X_train_num = train_df[num_features].to_numpy()\n",
    "X_test_num  = test_df[num_features].to_numpy()\n",
    "\n",
    "X_train_combined = hstack([X_train_tfidf, sp.csr_matrix(X_train_num)])\n",
    "X_test_combined  = hstack([X_test_tfidf,  sp.csr_matrix(X_test_num)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were the main points I did i wont include all the model steps. this is the one that got my best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "\n",
    "#----\n",
    "\n",
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_len = 256\n",
    "\n",
    "train_enc = tokenizer(\n",
    "    train_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "val_enc = tokenizer(\n",
    "    val_texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "test_enc = tokenizer(\n",
    "    test_df[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_len\n",
    ")\n",
    "\n",
    "\n",
    "#----------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deberta_v3_large_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    logging_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#------\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
